{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7151fcda",
   "metadata": {},
   "source": [
    "## Modelo de Detecção de Modelos de Detecção de Smishing\n",
    "\n",
    "Ankit Kumar Jain, B.B. Gupta,\n",
    "Rule-Based Framework for Detection of Smishing Messages in Mobile Environment,\n",
    "Procedia Computer Science,\n",
    "Volume 125,\n",
    "2018,\n",
    "Pages 617-623,\n",
    "ISSN 1877-0509,\n",
    "https://doi.org/10.1016/j.procs.2017.12.079.\n",
    "(https://www.sciencedirect.com/science/article/pii/S1877050917328478)\n",
    "Abstract: Smishing is a cyber-security attack, which utilizes Short Message Service (SMS) to steal personal credentials of mobile users. The trust level of users on their smart devices has attracted attackers for performing various mobile security attacks like Smishing. In this paper, we implement the rule-based data mining classification approach in the detection of smishing messages. The proposed approach identified nine rules which can efficiently filter smishing SMS from the genuine one. Further, our approach applies rule-based classification algorithms to train these outstanding rules. Since the SMS text messages are very short and generally written in Lingo language, we have used text normalization to convert them into standard form to obtain better rules. The performance of the proposed approach is evaluated, and it achieved more than 99% true negative rate. Furthermore, the proposed approach is very efficient for the detection of the zero hour attack too.\n",
    "Keywords: Smishing; Mobile Phishing; Data mining; Short messaging service; Machine learning\n",
    "\n",
    "> Reprodução de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71268bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import ruleset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75896f",
   "metadata": {},
   "source": [
    "#### 1. Pré processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08546b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "44e1910c-fd67-4581-b6fb-e1f3d53b744f",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni..."
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say..."
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though"
        ],
        [
         "5",
         "spam",
         "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv"
        ],
        [
         "6",
         "ham",
         "Even my brother is not like to speak with me. They treat me like aids patent."
        ],
        [
         "7",
         "ham",
         "As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune"
        ],
        [
         "8",
         "spam",
         "WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only."
        ],
        [
         "9",
         "spam",
         "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030"
        ],
        [
         "10",
         "ham",
         "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today."
        ],
        [
         "11",
         "spam",
         "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info"
        ],
        [
         "12",
         "spam",
         "URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18"
        ],
        [
         "13",
         "ham",
         "I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times."
        ],
        [
         "14",
         "ham",
         "I HAVE A DATE ON SUNDAY WITH WILL!!"
        ],
        [
         "15",
         "spam",
         "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL"
        ],
        [
         "16",
         "ham",
         "Oh k...i'm watching here:)"
        ],
        [
         "17",
         "ham",
         "Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet."
        ],
        [
         "18",
         "ham",
         "Fine if thatåÕs the way u feel. ThatåÕs the way its gota b"
        ],
        [
         "19",
         "spam",
         "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ì¼1.20 POBOXox36504W45WQ 16+"
        ],
        [
         "20",
         "ham",
         "Is that seriously how you spell his name?"
        ],
        [
         "21",
         "ham",
         "IÛ÷m going to try for 2 months ha ha only joking"
        ],
        [
         "22",
         "ham",
         "So Ì_ pay first lar... Then when is da stock comin..."
        ],
        [
         "23",
         "ham",
         "Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?"
        ],
        [
         "24",
         "ham",
         "Ffffffffff. Alright no way I can meet up with you sooner?"
        ],
        [
         "25",
         "ham",
         "Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol"
        ],
        [
         "26",
         "ham",
         "Lol your always so convincing."
        ],
        [
         "27",
         "ham",
         "Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?"
        ],
        [
         "28",
         "ham",
         "I'm back &amp; we're packing the car now, I'll let you know if there's room"
        ],
        [
         "29",
         "ham",
         "Ahhh. Work. I vaguely remember that! What does it feel like? Lol"
        ],
        [
         "30",
         "ham",
         "Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us"
        ],
        [
         "31",
         "ham",
         "Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't go there! Not doing too badly cheers. You? "
        ],
        [
         "32",
         "ham",
         "K tell me anything about you."
        ],
        [
         "33",
         "ham",
         "For fear of fainting with the of all that housework you just did? Quick have a cuppa"
        ],
        [
         "34",
         "spam",
         "Thanks for your subscription to Ringtone UK your mobile will be charged å£5/month Please confirm by replying YES or NO. If you reply NO you will not be charged"
        ],
        [
         "35",
         "ham",
         "Yup... Ok i go home look at the timings then i msg Ì_ again... Xuhui going to learn on 2nd may too but her lesson is at 8am"
        ],
        [
         "36",
         "ham",
         "Oops, I'll let you know when my roommate's done"
        ],
        [
         "37",
         "ham",
         "I see the letter B on my car"
        ],
        [
         "38",
         "ham",
         "Anything lor... U decide..."
        ],
        [
         "39",
         "ham",
         "Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!"
        ],
        [
         "40",
         "ham",
         "Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola"
        ],
        [
         "41",
         "ham",
         "Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love you my sweet Arabian steed ... Mmmmmm ... Yummy"
        ],
        [
         "42",
         "spam",
         "07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow"
        ],
        [
         "43",
         "ham",
         "WHO ARE YOU SEEING?"
        ],
        [
         "44",
         "ham",
         "Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches..."
        ],
        [
         "45",
         "ham",
         "No calls..messages..missed calls"
        ],
        [
         "46",
         "ham",
         "Didn't you get hep b immunisation in nigeria."
        ],
        [
         "47",
         "ham",
         "Fair enough, anything going on?"
        ],
        [
         "48",
         "ham",
         "Yeah hopefully, if tyler can't do it I could maybe ask around a bit"
        ],
        [
         "49",
         "ham",
         "U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'm not a weak sucker. Hospitals are for weak suckers."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5572
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the CSV dataset as a dataframe\n",
    "# Since pandas is already imported in cell 1, we can use it directly\n",
    "df = pandas.read_csv('SMSSpamCollectionDataset.csv', encoding='latin-1')\n",
    "df = df[['label', 'text']]\n",
    "\n",
    "# Display the first few rows to get a glimpse of the data\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "299f844b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005!\n",
      "Normalizado: free entry wkly comp win cup final tkts 21st may 2005 !\n"
     ]
    }
   ],
   "source": [
    "# Download dos recursos necessários do NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True) \n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def normalize_text(original_text) -> str:\n",
    "    '''\n",
    "    Recebe um SMS\n",
    "    \n",
    "    Retorna texto original normalizado (mais conservador para melhor performance)\n",
    "    '''\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = original_text.lower()\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize (método mais simples se houver problemas com punkt)\n",
    "    try:\n",
    "        words = nltk.word_tokenize(text)\n",
    "    except:\n",
    "        # Fallback para tokenização simples\n",
    "        words = text.split()\n",
    "    \n",
    "    # Remove stopwords apenas as mais comuns (mais conservador)\n",
    "    # IMPORTANTE: Preservar símbolos financeiros e matemáticos mesmo que sejam curtos\n",
    "    common_stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}\n",
    "    \n",
    "    # Símbolos financeiros e matemáticos importantes para detecção de spam/smishing\n",
    "    important_symbols = {\n",
    "        # Símbolos financeiros\n",
    "        '$', '£', '€', '¥', '₹', '¢', \n",
    "        # Símbolos matemáticos\n",
    "        '+', '-', '*', '/', '=', '<', '>', '≤', '≥', '≠', '±', '×', '÷',\n",
    "        # Outros símbolos importantes  \n",
    "        '%', '#', '@', '&', '!', '?'\n",
    "    }\n",
    "    \n",
    "    # Manter palavra se: não é stopword E (tem mais de 1 char OU é símbolo importante)\n",
    "    words = [word for word in words if word not in common_stopwords and (len(word) > 1 or word in important_symbols)]\n",
    "    \n",
    "    # NÃO aplicar stemming agressivo - manter palavras mais íntegras\n",
    "    # Preservar símbolos importantes e palavras relevantes\n",
    "    words = [word for word in words if len(word) > 2 or word in important_symbols]\n",
    "    \n",
    "    # Join words back into a string\n",
    "    normalized_text = ' '.join(words)\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "# Testar com um exemplo\n",
    "sample_text = \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005!\"\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Normalizado: {normalize_text(sample_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0678b",
   "metadata": {},
   "source": [
    "(debugando o por quê do símbolo de dolar estar sumindo e verificando que ainda funciona...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b4606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Original: 'Get $500 bonus!'\n",
      "2. Lowercase: 'get $500 bonus!'\n",
      "3. Remove spaces: 'get $500 bonus!'\n",
      "4. NLTK tokenize: ['get', '$', '500', 'bonus', '!']\n",
      "5. After stopwords removal: ['get', '$', '500', 'bonus', '!']\n",
      "6. After length filter (>2): ['get', '$', '500', 'bonus', '!']\n",
      "7. Final result: 'get $ 500 bonus !'\n",
      "--------------------------------------------------\n",
      "1. Original: 'Only $19.99 today!'\n",
      "2. Lowercase: 'only $19.99 today!'\n",
      "3. Remove spaces: 'only $19.99 today!'\n",
      "4. NLTK tokenize: ['only', '$', '19.99', 'today', '!']\n",
      "5. After stopwords removal: ['only', '$', '19.99', 'today', '!']\n",
      "6. After length filter (>2): ['only', '$', '19.99', 'today', '!']\n",
      "7. Final result: 'only $ 19.99 today !'\n",
      "--------------------------------------------------\n",
      "1. Original: 'Win £1000 prize!'\n",
      "2. Lowercase: 'win £1000 prize!'\n",
      "3. Remove spaces: 'win £1000 prize!'\n",
      "4. NLTK tokenize: ['win', '£1000', 'prize', '!']\n",
      "5. After stopwords removal: ['win', '£1000', 'prize', '!']\n",
      "6. After length filter (>2): ['win', '£1000', 'prize', '!']\n",
      "7. Final result: 'win £1000 prize !'\n",
      "--------------------------------------------------\n",
      "1. Original: 'Cost €250 only'\n",
      "2. Lowercase: 'cost €250 only'\n",
      "3. Remove spaces: 'cost €250 only'\n",
      "4. NLTK tokenize: ['cost', '€250', 'only']\n",
      "5. After stopwords removal: ['cost', '€250', 'only']\n",
      "6. After length filter (>2): ['cost', '€250', 'only']\n",
      "7. Final result: 'cost €250 only'\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Vamos rastrear passo a passo onde o $ está sendo perdido\n",
    "def debug_normalize_text(original_text):\n",
    "    print(f\"1. Original: '{original_text}'\")\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = original_text.lower()\n",
    "    print(f\"2. Lowercase: '{text}'\")\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    print(f\"3. Remove spaces: '{text}'\")\n",
    "    \n",
    "    # Tokenize\n",
    "    try:\n",
    "        words = nltk.word_tokenize(text)\n",
    "        print(f\"4. NLTK tokenize: {words}\")\n",
    "    except:\n",
    "        words = text.split()\n",
    "        print(f\"4. Simple split: {words}\")\n",
    "    \n",
    "    # Remove stopwords\n",
    "    common_stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}\n",
    "    important_symbols = {\n",
    "        # Símbolos financeiros\n",
    "        '$', '£', '€', '¥', '₹', '¢', \n",
    "        # Símbolos matemáticos\n",
    "        '+', '-', '*', '/', '=', '<', '>', '≤', '≥', '≠', '±', '×', '÷',\n",
    "        # Outros símbolos importantes  \n",
    "        '%', '#', '@', '&', '!', '?'\n",
    "    }\n",
    "    \n",
    "    # Manter palavra se: não é stopword E (tem mais de 1 char OU é símbolo importante)\n",
    "    words_after_stopwords = [word for word in words if word not in common_stopwords and (len(word) > 1 or word in important_symbols)]\n",
    "    print(f\"5. After stopwords removal: {words_after_stopwords}\")\n",
    "    \n",
    "    # Remove short words mas preserva símbolos importantes\n",
    "    words_after_length = [word for word in words_after_stopwords if len(word) > 2 or word in important_symbols]\n",
    "    print(f\"6. After length filter (>2): {words_after_length}\")\n",
    "    \n",
    "    result = ' '.join(words_after_length)\n",
    "    print(f\"7. Final result: '{result}'\")\n",
    "    print(\"-\" * 50)\n",
    "    return result\n",
    "\n",
    "# Testar com exemplos problemáticos\n",
    "test_cases = [\n",
    "    \"Get $500 bonus!\",\n",
    "    \"Only $19.99 today!\",\n",
    "    \"Win £1000 prize!\",\n",
    "    \"Cost €250 only\"\n",
    "]\n",
    "\n",
    "for test in test_cases:\n",
    "    debug_normalize_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbbc8bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTE COM SÍMBOLOS MATEMÁTICOS E FINANCEIROS ===\n",
      "\n",
      "1. Original: Get $500 + 20% bonus = $600 total!\n",
      "   Normalizado: get $ 500 + % bonus = $ 600 total !\n",
      "   Símbolos preservados: $(financial), +(mathematical), =(mathematical), %(other), !(other)\n",
      "\n",
      "2. Original: Discount: 50% - 10% = 40% off today!\n",
      "   Normalizado: discount % - % = % off today !\n",
      "   Símbolos preservados: -(mathematical), =(mathematical), %(other), !(other)\n",
      "\n",
      "3. Original: Calculate: 2 + 2 = 4, 10 * 5 = 50\n",
      "   Normalizado: calculate + = * =\n",
      "   Símbolos preservados: +(mathematical), *(mathematical), =(mathematical)\n",
      "\n",
      "4. Original: Price: £100 > £80 (save £20!)\n",
      "   Normalizado: price £100 > £80 save £20 !\n",
      "   Símbolos preservados: £(financial), >(mathematical), !(other)\n",
      "\n",
      "5. Original: Rate: 5% < 10% but > 2%\n",
      "   Normalizado: rate % < % > %\n",
      "   Símbolos preservados: <(mathematical), >(mathematical), %(other)\n",
      "\n",
      "6. Original: Win €250 × 2 = €500 prize!\n",
      "   Normalizado: win €250 × = €500 prize !\n",
      "   Símbolos preservados: €(financial), =(mathematical), ×(mathematical), !(other)\n",
      "\n",
      "7. Original: Cost: $19.99 ÷ 2 = $9.99 each\n",
      "   Normalizado: cost $ 19.99 ÷ = $ 9.99 each\n",
      "   Símbolos preservados: $(financial), =(mathematical), ÷(mathematical)\n"
     ]
    }
   ],
   "source": [
    "# Testar com exemplos que contêm símbolos matemáticos e financeiros\n",
    "math_test_cases = [\n",
    "    \"Get $500 + 20% bonus = $600 total!\",\n",
    "    \"Discount: 50% - 10% = 40% off today!\",\n",
    "    \"Calculate: 2 + 2 = 4, 10 * 5 = 50\",\n",
    "    \"Price: £100 > £80 (save £20!)\",\n",
    "    \"Rate: 5% < 10% but > 2%\",\n",
    "    \"Win €250 × 2 = €500 prize!\",\n",
    "    \"Cost: $19.99 ÷ 2 = $9.99 each\"\n",
    "]\n",
    "\n",
    "print(\"=== TESTE COM SÍMBOLOS MATEMÁTICOS E FINANCEIROS ===\")\n",
    "for i, text in enumerate(math_test_cases, 1):\n",
    "    normalized = normalize_text(text)\n",
    "    print(f\"\\n{i}. Original: {text}\")\n",
    "    print(f\"   Normalizado: {normalized}\")\n",
    "    \n",
    "    # Verificar se símbolos importantes foram preservados\n",
    "    important_symbols_in_text = {\n",
    "        'financial': ['$', '£', '€', '¥', '₹', '¢'],\n",
    "        'mathematical': ['+', '-', '*', '/', '=', '<', '>', '≤', '≥', '≠', '±', '×', '÷'],\n",
    "        'other': ['%', '#', '@', '&', '!', '?']\n",
    "    }\n",
    "    \n",
    "    found_symbols = []\n",
    "    for category, symbols in important_symbols_in_text.items():\n",
    "        for sym in symbols:\n",
    "            if sym in text and sym in normalized:\n",
    "                found_symbols.append(f\"{sym}({category})\")\n",
    "            elif sym in text and sym not in normalized:\n",
    "                found_symbols.append(f\"❌{sym}({category})\")\n",
    "    \n",
    "    if found_symbols:\n",
    "        print(f\"   Símbolos preservados: {', '.join(found_symbols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4e3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                     normalized_text  \n",
      "0  until jurong point crazy available only bugis ...  \n",
      "1                         lar ... joking wif oni ...  \n",
      "2  free entry wkly comp win cup final tkts 21st m...  \n",
      "3         dun say early hor ... already then say ...  \n",
      "4    nah n't think goes usf lives around here though  \n"
     ]
    }
   ],
   "source": [
    "# Apply the normalize_text function to the text column\n",
    "df['normalized_text'] = df['text'].apply(normalize_text)\n",
    "\n",
    "# Display the first few rows to see the normalized text\n",
    "print(df[['text', 'normalized_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae67821",
   "metadata": {},
   "source": [
    "### 2. Extração de features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "101118a4",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after adding rule columns: (5572, 12)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "normalized_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rule1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rule2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rule3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rule4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rule5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rule6",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rule7",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rule8",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rule9",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "6e93a384-4c0a-4911-806b-7e694731540f",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...",
         "until jurong point crazy available only bugis great world buffet ... cine there got amore wat ...",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0"
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni...",
         "lar ... joking wif oni ...",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0"
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's",
         "free entry wkly comp win cup final tkts 21st may 2005. text 87121 receive entry question std txt rate & apply 08452810075over18",
         "0",
         "0",
         "0",
         "1",
         "1",
         "0",
         "0",
         "1",
         "0"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say...",
         "dun say early hor ... already then say ...",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0"
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though",
         "nah n't think goes usf lives around here though",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>rule1</th>\n",
       "      <th>rule2</th>\n",
       "      <th>rule3</th>\n",
       "      <th>rule4</th>\n",
       "      <th>rule5</th>\n",
       "      <th>rule6</th>\n",
       "      <th>rule7</th>\n",
       "      <th>rule8</th>\n",
       "      <th>rule9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>until jurong point crazy available only bugis ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>lar ... joking wif oni ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry wkly comp win cup final tkts 21st m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>dun say early hor ... already then say ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah n't think goes usf lives around here though</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                     normalized_text  rule1  rule2  rule3  \\\n",
       "0  until jurong point crazy available only bugis ...      0      0      0   \n",
       "1                         lar ... joking wif oni ...      0      0      0   \n",
       "2  free entry wkly comp win cup final tkts 21st m...      0      0      0   \n",
       "3         dun say early hor ... already then say ...      0      0      0   \n",
       "4    nah n't think goes usf lives around here though      0      0      0   \n",
       "\n",
       "   rule4  rule5  rule6  rule7  rule8  rule9  \n",
       "0      0      1      0      0      1      0  \n",
       "1      0      0      0      0      1      0  \n",
       "2      1      1      0      0      1      0  \n",
       "3      0      0      0      0      1      0  \n",
       "4      0      0      0      0      0      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply each rule function from the ruleset module to create new columns\n",
    "df['rule1'] = df['normalized_text'].apply(ruleset.rule1)\n",
    "df['rule2'] = df['normalized_text'].apply(ruleset.rule2)\n",
    "df['rule3'] = df['normalized_text'].apply(ruleset.rule3)\n",
    "df['rule4'] = df['normalized_text'].apply(ruleset.rule4)\n",
    "df['rule5'] = df['normalized_text'].apply(ruleset.rule5)\n",
    "df['rule6'] = df['normalized_text'].apply(ruleset.rule6)\n",
    "df['rule7'] = df['normalized_text'].apply(ruleset.rule7)\n",
    "df['rule8'] = df['normalized_text'].apply(ruleset.rule8)\n",
    "df['rule9'] = df['normalized_text'].apply(ruleset.rule9)\n",
    "\n",
    "# Display the dataframe with all rule columns\n",
    "print(\"Shape after adding rule columns:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfb4334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Results:\n",
      "Accuracy: 98.92%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       965\n",
      "           1       0.97      0.95      0.96       150\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.97      0.98      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[960   5]\n",
      " [  7 143]]\n",
      "\n",
      "Feature Importance:\n",
      "  Feature  Importance\n",
      "3   rule4    0.902025\n",
      "4   rule5    0.045402\n",
      "0   rule1    0.016887\n",
      "1   rule2    0.009734\n",
      "2   rule3    0.009129\n",
      "6   rule7    0.006816\n",
      "5   rule6    0.005381\n",
      "7   rule8    0.004627\n",
      "8   rule9    0.000000\n",
      "\n",
      "True Negative Rate: 99.48%\n",
      "True Positive Rate (Sensitivity/Recall): 95.33%\n",
      "False Positive Rate: 0.52%\n",
      "False Negative Rate: 4.67%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Remove the problematic environment variable\n",
    "if 'MPLBACKEND' in os.environ:\n",
    "    del os.environ['MPLBACKEND']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Convert labels to binary values (ham=0, spam=1)\n",
    "df['binary_label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Extract features (all rule columns) and target variable\n",
    "X = df[['rule1', 'rule2', 'rule3', 'rule4', 'rule5', 'rule6', 'rule7', 'rule8', 'rule9']]\n",
    "y = df['binary_label']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train a Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Decision Tree Classifier Results:\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate feature importances\n",
    "feature_importances = dt_classifier.feature_importances_\n",
    "feature_names = X.columns\n",
    "importance_df = pandas.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Print true negative rate as mentioned in the paper\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "tnr = tn / (tn + fp)\n",
    "print(f\"\\nTrue Negative Rate: {tnr * 100:.2f}%\")\n",
    "print(f\"True Positive Rate (Sensitivity/Recall): {tp / (tp + fn) * 100:.2f}%\")\n",
    "print(f\"False Positive Rate: {fp / (fp + tn) * 100:.2f}%\")\n",
    "print(f\"False Negative Rate: {fn / (fn + tp) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc807737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Results:\n",
      "Accuracy: 97.78%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      3860\n",
      "           1       0.95      0.88      0.91       597\n",
      "\n",
      "    accuracy                           0.98      4457\n",
      "   macro avg       0.97      0.94      0.95      4457\n",
      "weighted avg       0.98      0.98      0.98      4457\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3832   28]\n",
      " [  71  526]]\n",
      "\n",
      "Feature Importance:\n",
      "  Feature  Importance\n",
      "3   rule4    0.902025\n",
      "4   rule5    0.045402\n",
      "0   rule1    0.016887\n",
      "1   rule2    0.009734\n",
      "2   rule3    0.009129\n",
      "6   rule7    0.006816\n",
      "5   rule6    0.005381\n",
      "7   rule8    0.004627\n",
      "8   rule9    0.000000\n",
      "\n",
      "True Negative Rate: 99.27%\n",
      "True Positive Rate (Sensitivity/Recall): 88.11%\n",
      "False Positive Rate: 0.73%\n",
      "False Negative Rate: 11.89%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data\n",
    "y_pred = dt_classifier.predict(X_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "class_report = classification_report(y_train, y_pred)\n",
    "conf_matrix = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Decision Tree Classifier Results:\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate feature importances\n",
    "feature_importances = dt_classifier.feature_importances_\n",
    "feature_names = X.columns\n",
    "importance_df = pandas.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Print true negative rate as mentioned in the paper\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "tnr = tn / (tn + fp)\n",
    "print(f\"\\nTrue Negative Rate: {tnr * 100:.2f}%\")\n",
    "print(f\"True Positive Rate (Sensitivity/Recall): {tp / (tp + fn) * 100:.2f}%\")\n",
    "print(f\"False Positive Rate: {fp / (fp + tn) * 100:.2f}%\")\n",
    "print(f\"False Negative Rate: {fn / (fn + tp) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "241c2064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANÁLISE DE OVERFITTING ===\n",
      "\n",
      "Validação Cruzada (5-fold):\n",
      "Scores: [0.9793722  0.9838565  0.97576302 0.97666068 0.97486535]\n",
      "Média: 0.9781 (+/- 0.0065)\n",
      "Desvio padrão: 0.0032\n",
      "✅ Desvio padrão baixo - modelo parece estável\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "## Análise de Overfitting\n",
    "\n",
    "# Vamos usar validação cruzada para verificar se há overfitting\n",
    "from sklearn.model_selection import cross_val_score, validation_curve, learning_curve\n",
    "import numpy as np\n",
    "\n",
    "# 1. Validação Cruzada com 5 folds\n",
    "print(\"=== ANÁLISE DE OVERFITTING ===\\n\")\n",
    "\n",
    "# Validação cruzada\n",
    "cv_scores = cross_val_score(dt_classifier, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"Validação Cruzada (5-fold):\")\n",
    "print(f\"Scores: {cv_scores}\")\n",
    "print(f\"Média: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "print(f\"Desvio padrão: {cv_scores.std():.4f}\")\n",
    "\n",
    "# Se o desvio padrão for muito alto, pode indicar overfitting\n",
    "if cv_scores.std() > 0.02:\n",
    "    print(\"⚠️  ALERTA: Alto desvio padrão pode indicar overfitting!\")\n",
    "else:\n",
    "    print(\"✅ Desvio padrão baixo - modelo parece estável\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d1b00e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANÁLISE DAS FEATURES ===\n",
      "Distribuição das regras (quantos SMS triggeram cada regra):\n",
      "rule1: 357/5572 (6.4%)\n",
      "rule2: 777/5572 (13.9%)\n",
      "rule3: 318/5572 (5.7%)\n",
      "rule4: 711/5572 (12.8%)\n",
      "rule5: 1844/5572 (33.1%)\n",
      "rule6: 146/5572 (2.6%)\n",
      "rule7: 199/5572 (3.6%)\n",
      "rule8: 1509/5572 (27.1%)\n",
      "rule9: 0/5572 (0.0%)\n",
      "\n",
      "SMS que não triggeram NENHUMA regra: 2516\n",
      "Destes, quantos são spam: 14\n",
      "Destes, quantos são ham: 2502\n",
      "\n",
      "Correlações mais altas entre regras:\n",
      "rule2 vs rule4: 0.402\n",
      "rule3 vs rule4: 0.560\n",
      "rule3 vs rule8: 0.348\n",
      "rule4 vs rule5: 0.437\n",
      "rule4 vs rule8: 0.383\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 2. Análise da Distribuição das Features\n",
    "print(\"\\n=== ANÁLISE DAS FEATURES ===\")\n",
    "\n",
    "# Verificar distribuição das regras\n",
    "feature_distribution = df[['rule1', 'rule2', 'rule3', 'rule4', 'rule5', 'rule6', 'rule7', 'rule8', 'rule9']].sum()\n",
    "total_samples = len(df)\n",
    "\n",
    "print(\"Distribuição das regras (quantos SMS triggeram cada regra):\")\n",
    "for rule, count in feature_distribution.items():\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"{rule}: {count}/{total_samples} ({percentage:.1f}%)\")\n",
    "\n",
    "# Verificar quantos SMS não triggeraram nenhuma regra\n",
    "no_rules_triggered = df[(df[['rule1', 'rule2', 'rule3', 'rule4', 'rule5', 'rule6', 'rule7', 'rule8', 'rule9']].sum(axis=1) == 0)]\n",
    "print(f\"\\nSMS que não triggeram NENHUMA regra: {len(no_rules_triggered)}\")\n",
    "print(f\"Destes, quantos são spam: {len(no_rules_triggered[no_rules_triggered['label'] == 'spam'])}\")\n",
    "print(f\"Destes, quantos são ham: {len(no_rules_triggered[no_rules_triggered['label'] == 'ham'])}\")\n",
    "\n",
    "# Verificar correlação entre regras\n",
    "correlation_matrix = df[['rule1', 'rule2', 'rule3', 'rule4', 'rule5', 'rule6', 'rule7', 'rule8', 'rule9']].corr()\n",
    "print(f\"\\nCorrelações mais altas entre regras:\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.3:  # Correlação moderada ou alta\n",
    "            print(f\"{correlation_matrix.columns[i]} vs {correlation_matrix.columns[j]}: {corr_value:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "985dec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANÁLISE DETALHADA DA RULE4 ===\n",
      "Distribuição Rule4 vs Label:\n",
      "label   ham  spam\n",
      "rule4            \n",
      "0      4766    95\n",
      "1        59   652\n",
      "\n",
      "Se usássemos APENAS a Rule4:\n",
      "Acurácia: 0.9724 (97.24%)\n",
      "\n",
      "Rule4 detecta 652/747 spam (87.3%)\n",
      "Rule4 é triggerrada por 59/4825 ham (1.2%)\n",
      "⚠️  Rule4 pode estar sendo muito específica para spam!\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 3. Análise específica da Rule4 (dominante)\n",
    "print(\"\\n=== ANÁLISE DETALHADA DA RULE4 ===\")\n",
    "\n",
    "# Analisar a performance da rule4 sozinha\n",
    "rule4_analysis = df.groupby(['rule4', 'label']).size().unstack(fill_value=0)\n",
    "print(\"Distribuição Rule4 vs Label:\")\n",
    "print(rule4_analysis)\n",
    "\n",
    "# Calcular métricas se usássemos apenas a rule4\n",
    "rule4_only_accuracy = ((rule4_analysis.loc[0, 'ham'] + rule4_analysis.loc[1, 'spam']) / len(df))\n",
    "print(f\"\\nSe usássemos APENAS a Rule4:\")\n",
    "print(f\"Acurácia: {rule4_only_accuracy:.4f} ({rule4_only_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Verificar quantos spam/ham triggeraram rule4\n",
    "spam_with_rule4 = len(df[(df['label'] == 'spam') & (df['rule4'] == 1)])\n",
    "total_spam = len(df[df['label'] == 'spam'])\n",
    "ham_with_rule4 = len(df[(df['label'] == 'ham') & (df['rule4'] == 1)])\n",
    "total_ham = len(df[df['label'] == 'ham'])\n",
    "\n",
    "print(f\"\\nRule4 detecta {spam_with_rule4}/{total_spam} spam ({spam_with_rule4/total_spam*100:.1f}%)\")\n",
    "print(f\"Rule4 é triggerrada por {ham_with_rule4}/{total_ham} ham ({ham_with_rule4/total_ham*100:.1f}%)\")\n",
    "\n",
    "# Isso mostra se a rule4 é muito específica\n",
    "if ham_with_rule4/total_ham < 0.02:  # Menos de 2% dos ham triggeram rule4\n",
    "    print(\"⚠️  Rule4 pode estar sendo muito específica para spam!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c8df901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TESTE SEM RULE4 ===\n",
      "Acurácia SEM Rule4: 0.9354 (93.54%)\n",
      "Queda de performance: 4.24 pontos percentuais\n",
      "\n",
      "Nova distribuição de importância (sem rule4):\n",
      "rule3: 0.497\n",
      "rule5: 0.225\n",
      "rule2: 0.168\n",
      "rule8: 0.035\n",
      "rule1: 0.029\n",
      "rule7: 0.029\n",
      "rule6: 0.017\n",
      "rule9: 0.000\n",
      "\n",
      "Validação cruzada sem rule4: 0.9363 (+/- 0.0073)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 4. Teste sem a Rule4 dominante\n",
    "print(\"\\n=== TESTE SEM RULE4 ===\")\n",
    "\n",
    "# Treinar modelo sem a rule4\n",
    "X_without_rule4 = df[['rule1', 'rule2', 'rule3', 'rule5', 'rule6', 'rule7', 'rule8', 'rule9']]\n",
    "y = df['binary_label']\n",
    "\n",
    "# Split dos dados\n",
    "X_train_no4, X_test_no4, y_train_no4, y_test_no4 = train_test_split(\n",
    "    X_without_rule4, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Treinar novo modelo\n",
    "dt_no_rule4 = DecisionTreeClassifier(random_state=42)\n",
    "dt_no_rule4.fit(X_train_no4, y_train_no4)\n",
    "\n",
    "# Fazer predições\n",
    "y_pred_no4 = dt_no_rule4.predict(X_test_no4)\n",
    "accuracy_no4 = accuracy_score(y_test_no4, y_pred_no4)\n",
    "\n",
    "print(f\"Acurácia SEM Rule4: {accuracy_no4:.4f} ({accuracy_no4*100:.2f}%)\")\n",
    "print(f\"Queda de performance: {(accuracy - accuracy_no4)*100:.2f} pontos percentuais\")\n",
    "\n",
    "# Feature importance sem rule4\n",
    "feature_imp_no4 = dt_no_rule4.feature_importances_\n",
    "feature_names_no4 = X_without_rule4.columns\n",
    "importance_df_no4 = pandas.DataFrame({'Feature': feature_names_no4, 'Importance': feature_imp_no4})\n",
    "importance_df_no4 = importance_df_no4.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nNova distribuição de importância (sem rule4):\")\n",
    "for idx, row in importance_df_no4.iterrows():\n",
    "    print(f\"{row['Feature']}: {row['Importance']:.3f}\")\n",
    "\n",
    "# Validação cruzada sem rule4\n",
    "cv_scores_no4 = cross_val_score(dt_no_rule4, X_without_rule4, y, cv=5, scoring='accuracy')\n",
    "print(f\"\\nValidação cruzada sem rule4: {cv_scores_no4.mean():.4f} (+/- {cv_scores_no4.std() * 2:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9fef097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONCLUSÕES SOBRE OVERFITTING ===\n",
      "✅ EVIDÊNCIAS CONTRA OVERFITTING:\n",
      "• Validação cruzada estável (std = 0.32%)\n",
      "• Modelo sem rule4 ainda tem 93.54% de acurácia\n",
      "• 2516 SMS não triggeram nenhuma regra, sendo 2502 ham e apenas 14 spam\n",
      "\n",
      "⚠️  POSSÍVEIS PREOCUPAÇÕES:\n",
      "• Rule4 domina com 90.2% de importância\n",
      "• Rule4 sozinha já dá 97.24% de acurácia\n",
      "• Rule4 triggera apenas 1.2% dos ham vs 87.3% dos spam\n",
      "• Rule9 (email) nunca é triggerrada (0% dos casos)\n",
      "\n",
      "🔍 INTERPRETAÇÃO:\n",
      "• Não há overfitting clássico (modelo generaliza bem)\n",
      "• Mas há DEPENDÊNCIA EXCESSIVA da rule4 (números de telefone)\n",
      "• Isso pode ser um problema de generalização para outros tipos de spam\n",
      "• Rule4 pode estar sendo um 'atalho' muito específico para este dataset\n",
      "\n",
      "📊 RESUMO NUMÉRICO:\n",
      "• Acurácia original: 97.78%\n",
      "• Acurácia sem rule4: 93.54%\n",
      "• Contribuição da rule4: 4.24 pontos percentuais\n",
      "• Rule4 = 4.3% da performance total\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 5. Conclusões sobre Overfitting\n",
    "print(\"\\n=== CONCLUSÕES SOBRE OVERFITTING ===\")\n",
    "\n",
    "print(\"✅ EVIDÊNCIAS CONTRA OVERFITTING:\")\n",
    "print(\"• Validação cruzada estável (std = 0.32%)\")\n",
    "print(\"• Modelo sem rule4 ainda tem 93.54% de acurácia\")\n",
    "print(\"• 2516 SMS não triggeram nenhuma regra, sendo 2502 ham e apenas 14 spam\")\n",
    "\n",
    "print(\"\\n⚠️  POSSÍVEIS PREOCUPAÇÕES:\")\n",
    "print(\"• Rule4 domina com 90.2% de importância\")\n",
    "print(\"• Rule4 sozinha já dá 97.24% de acurácia\")\n",
    "print(\"• Rule4 triggera apenas 1.2% dos ham vs 87.3% dos spam\")\n",
    "print(\"• Rule9 (email) nunca é triggerrada (0% dos casos)\")\n",
    "\n",
    "print(\"\\n🔍 INTERPRETAÇÃO:\")\n",
    "print(\"• Não há overfitting clássico (modelo generaliza bem)\")\n",
    "print(\"• Mas há DEPENDÊNCIA EXCESSIVA da rule4 (números de telefone)\")\n",
    "print(\"• Isso pode ser um problema de generalização para outros tipos de spam\")\n",
    "print(\"• Rule4 pode estar sendo um 'atalho' muito específico para este dataset\")\n",
    "\n",
    "print(f\"\\n📊 RESUMO NUMÉRICO:\")\n",
    "print(f\"• Acurácia original: {accuracy*100:.2f}%\")\n",
    "print(f\"• Acurácia sem rule4: {accuracy_no4*100:.2f}%\")\n",
    "print(f\"• Contribuição da rule4: {(accuracy - accuracy_no4)*100:.2f} pontos percentuais\")\n",
    "print(f\"• Rule4 = {(accuracy - accuracy_no4)/accuracy*100:.1f}% da performance total\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
